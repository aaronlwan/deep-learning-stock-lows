{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0j5ab1cNjhkJ",
        "-ZdkaS4p-TSC",
        "FRLcfpyoixTP",
        "M8gPJDKE9gg2",
        "JqcQs0zv3rmW"
      ],
      "toc_visible": true,
      "mount_file_id": "19jTFkWDq4eQdxbOdg_mo8nvlJfBpBT6_",
      "authorship_tag": "ABX9TyPXpbs2PlCviGahdsb7ezxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronlwan/deep-learning-stock-lows/blob/main/Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVJLbCZPkkK-"
      },
      "outputs": [],
      "source": [
        "# Import Modules\n",
        "!pip install yfinance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import math\n",
        "from tensorflow.python.framework import ops\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "765D6wYFbq-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data"
      ],
      "metadata": {
        "id": "0j5ab1cNjhkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Data Within Window (min-max)\n",
        "def normalize_in_window(data):\n",
        "  new_data = []\n",
        "  for j in range(len(data)):\n",
        "    prices = []\n",
        "    volumes = []\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1) % 5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "    maxprice = max(prices)\n",
        "    minprice = min(prices)\n",
        "    maxvol = max(volumes)\n",
        "    minvol = min(volumes)\n",
        "    newrow = []\n",
        "    price_index = 0\n",
        "    vol_index = 0\n",
        "    for k in range(len(data[j])):\n",
        "      if (k + 1) % 5 == 0:\n",
        "        newrow.append((volumes[vol_index] - minvol)/(maxvol-minvol))\n",
        "        vol_index += 1\n",
        "      else:\n",
        "        newrow.append((prices[price_index] - minprice)/(maxprice - minprice))\n",
        "        price_index += 1\n",
        "    newrow.append(maxprice)\n",
        "    newrow.append(minprice)\n",
        "    new_data.append(newrow)\n",
        "  data = np.array(new_data)\n",
        "  return data\n",
        "\n",
        "# Normalize Over the Training Timeframe (min-max)\n",
        "def normalize_all(data, train_window):\n",
        "  prices = []\n",
        "  volumes = []\n",
        "  for j in range(train_window):\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "  maxprice = max(prices)\n",
        "  minprice = min(prices)\n",
        "  minvol = min(volumes)\n",
        "  maxvol = max(volumes)\n",
        "  print('Max Price:', maxprice)\n",
        "  print('Min Price:', minprice)\n",
        "  new_data = []\n",
        "  for k in range(len(data)):\n",
        "    row = []\n",
        "    for i in range(len(data[k])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        row.append((data[k][i]-minvol)/(maxvol-minvol))\n",
        "      else:\n",
        "        row.append((data[k][i]-minprice)/(maxprice-minprice))\n",
        "    new_data.append(row)\n",
        "  return np.array(new_data)\n",
        "\n",
        "# Standardize over the whole Training Timeframe (0 mean, 1std)\n",
        "def standardize_all(data):\n",
        "  prices = []\n",
        "  volumes = []\n",
        "  for j in range(len(data)):\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "  meanprice = np.average(np.array(prices))\n",
        "  stdprice = np.std(np.array(prices))\n",
        "  meanvol = np.sum(np.array(volumes))/len(prices)\n",
        "  stdvol = np.std(np.array(volumes))\n",
        "  print('Mean Price:', meanprice)\n",
        "  print('STD Price:', stdprice)\n",
        "  new_data = []\n",
        "  for k in range(len(data)):\n",
        "    row = []\n",
        "    for i in range(len(data[k])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        row.append((data[k][i]-meanvol)/(stdvol))\n",
        "      else:\n",
        "        row.append((data[k][i]-meanprice)/(stdprice))\n",
        "    new_data.append(row)\n",
        "  return np.array(new_data)\n",
        "\n",
        "def log_normalization(data):\n",
        "  return np.log(data)\n",
        "\n",
        "\n",
        "\n",
        "# Normalize Over the Training Timeframe w/ vwap (min-max)\n",
        "def normalize_all_vwap(data, train_window):\n",
        "  prices = []\n",
        "  volumes = []\n",
        "  vwaps = []\n",
        "  for j in range(train_window):\n",
        "    index = 0\n",
        "    for i in range(len(data[j])):\n",
        "      if (index + 1)%5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "        index += 1\n",
        "      elif (index + 1)%6 == 0:\n",
        "        vwaps.append(data[j][i])\n",
        "        index = 0\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "        index += 1\n",
        "  maxprice = max(prices)\n",
        "  minprice = min(prices)\n",
        "  minvol = min(volumes)\n",
        "  maxvol = max(volumes)\n",
        "  maxvwap = max(vwaps)\n",
        "  minvwap = min(vwaps)\n",
        "  print('Max Price:', maxprice)\n",
        "  print('Min Price:', minprice)\n",
        "  new_data = []\n",
        "  for k in range(len(data)):\n",
        "    row = []\n",
        "    index = 0\n",
        "    for i in range(len(data[k])):\n",
        "      if (index + 1)%5 == 0:\n",
        "        row.append((data[k][i]-minvol)/(maxvol-minvol))\n",
        "        index += 1\n",
        "      elif (index + 1)%6 == 0:\n",
        "        row.append((data[k][i]-minvwap)/(maxvwap-minvwap))\n",
        "        index = 0\n",
        "      else:\n",
        "        row.append((data[k][i]-minprice)/(maxprice-minprice))\n",
        "        index += 1\n",
        "    new_data.append(row)\n",
        "  return np.array(new_data)\n"
      ],
      "metadata": {
        "id": "5zYVw9RMtwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mine other features"
      ],
      "metadata": {
        "id": "-ZdkaS4p-TSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "from ta.volume import VolumeWeightedAveragePrice\n",
        "# vwap: volume weighted price average, used by many institutions to determine when to enter a position\n",
        "def vwap_hour_to_hour(data):\n",
        "  # Convert numpy array to df so we can use TA library\n",
        "  data = pd.DataFrame(data, columns=['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "  data['vwap'] = VolumeWeightedAveragePrice(high=data['High'], low=data['Low'], close=data[\"Close\"], volume=data['Volume'], window=3, fillna=True).volume_weighted_average_price()\n",
        "  return data"
      ],
      "metadata": {
        "id": "pbKk1oWj-kjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### yfinance Data Processsing"
      ],
      "metadata": {
        "id": "mEMYBWjijVrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data From Yfinance\n",
        "def getData(ticker, input_interval, input_candles, output_interval):\n",
        "  # Load Input Data\n",
        "  # Input Data Columns: Date, Open, High, Low, Close, Volume\n",
        "  input = yf.download(ticker, interval=input_interval)\n",
        "  input = np.array(pd.DataFrame({'Date': input.index, 'Open': input.values[:, 0], 'High': input.values[:, 1],\n",
        "                         'Low': input.values[:, 2], 'Close': input.values[:, 3],\n",
        "                         'Volume': input.values[:, 5]}))\n",
        "  # Output Data Colums: Date, Low\n",
        "  # Load Output Data\n",
        "  output = yf.download(ticker, interval=output_interval)\n",
        "  output = np.array(pd.DataFrame({'Date': output.index, 'Low': output.values[:, 2]}))\n",
        "  \n",
        "  # Match Input w/ Output\n",
        "  # Matched Data Columns: Repeat (Date, Open, High, Low, Close, Volume) for each input candle, Output Low\n",
        "  data = []\n",
        "  input_dates = list(input[:, 0])\n",
        "  for row in output:\n",
        "    data_row = []\n",
        "    outdate = row[0]\n",
        "    try:\n",
        "      input_index = input_dates.index(outdate)\n",
        "      # Collect the input_candles preceeding the outdate\n",
        "      for i in range(input_candles):\n",
        "        candle = input[input_index - 1 - i][1:]\n",
        "        for item in candle:\n",
        "          data_row.append(item)\n",
        "      data_row.append(row[1])\n",
        "      data.append(data_row)\n",
        "    except: \n",
        "      pass\n",
        "  return np.array(data)"
      ],
      "metadata": {
        "id": "8uxA_LK6wHSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data\n",
        "data = getData(\"SPY\", \"1d\", 15, \"1d\")\n",
        "print(data.shape)\n",
        "data = normalize_all(data, 7000)"
      ],
      "metadata": {
        "id": "Ug3supUgttdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intraday Data Processing"
      ],
      "metadata": {
        "id": "DzgorHNniX8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intraday Process (5min to Predict 1hr)\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/5min.csv', header=None))\n",
        "output_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hr.csv', header=None))\n",
        "data = []\n",
        "deleted = []\n",
        "i = 0\n",
        "output_data = output_data[1:]\n",
        "index = 0\n",
        "while True:\n",
        "  try:\n",
        "    # Select 24 candles of data\n",
        "    selection = input_data[i: i+24][:, 1:]\n",
        "    last_timestamp = input_data[i: i+24][:, 0][23]\n",
        "    # Check last candle, if it doesn't match up we just omit from dataset b/c we have a lot of datapoints\n",
        "    if int(last_timestamp[14:16]) == 55:\n",
        "      selection = list(selection.flatten())\n",
        "      selection.append(output_data[index][1])\n",
        "      data.append(selection)\n",
        "      i += 12\n",
        "    else:\n",
        "      deleted.append(index)\n",
        "      difference = int(last_timestamp[14:16]) + 60 - 55\n",
        "      i += int(12 - difference/5)\n",
        "    index += 1\n",
        "  except:\n",
        "    break\n",
        "\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "print(deleted)\n",
        "data = normalize_all(data, 33000)"
      ],
      "metadata": {
        "id": "S5gNGL7icrih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1hr to predict 1d\n",
        "from pandas import Timestamp\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinput.csv', header=None))\n",
        "output = yf.download('SPY', interval='1d')\n",
        "output_data = np.array(pd.DataFrame({'Date': output.index, 'Low': output.values[:, 2]}))\n",
        "outdates = list(output_data[:, 0])\n",
        "start = outdates.index(Timestamp('2005-01-05 00:00:00'))\n",
        "output_data = output_data[start:]\n",
        "data = []\n",
        "deleted = []\n",
        "i = 0\n",
        "index = 0\n",
        "while True:\n",
        "  try:\n",
        "    # Select 24 candles of data\n",
        "    selection = input_data[i: i+24][:, 1:]\n",
        "    last_timestamp = input_data[i: i+24][:, 0][23]\n",
        "    # Check last candle, if it doesn't match up we just omit from dataset b/c we have a lot of datapoints\n",
        "    if int(last_timestamp[11:13]) == 19:\n",
        "      selection = list(selection.flatten())\n",
        "      selection.append(output_data[index][1])\n",
        "      data.append(selection)\n",
        "      i += 12\n",
        "    else:\n",
        "      deleted.append(index)\n",
        "      difference = int(last_timestamp[11:13]) - 7\n",
        "      i += int(12 - difference)\n",
        "    index += 1\n",
        "  except:\n",
        "    break\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "data = log_normalization(data)"
      ],
      "metadata": {
        "id": "w9eJrL2-1V7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1hr to predict 1hr\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinputandoutput.csv', header=None))\n",
        "#input_data = np.array(vwap_hour_to_hour(input_data))\n",
        "output_data = input_data[:, 3]\n",
        "data = []\n",
        "index = 0\n",
        "output_data = output_data[30:]\n",
        "for i in range(len(output_data)):\n",
        "  row = input_data[i: i+30, 1:]\n",
        "  row = row.flatten()\n",
        "  row = list(row)\n",
        "  row.append(output_data[i])\n",
        "  data.append(row)\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "data = normalize_all(data, 66000)"
      ],
      "metadata": {
        "id": "Kz6FLXvJn0T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data"
      ],
      "metadata": {
        "id": "FRLcfpyoixTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Train/Test based on time frame (simulates predicting the future using past data)\n",
        "x = data[:, [i for i in range(75)]]\n",
        "y = np.transpose([data[:, 75]])\n",
        "X_train, X_test, Y_train, Y_test = np.transpose(x[:7000]), np.transpose(x[7000:]), np.transpose(y[:7000]), np.transpose(y[7000:])"
      ],
      "metadata": {
        "id": "piCaaerZcoBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Layer RNN"
      ],
      "metadata": {
        "id": "M8gPJDKE9gg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_rnn(X, d):\n",
        "  # Each window is a column of X\n",
        "  new_X = []\n",
        "  for i in range(len(X[0])):\n",
        "    col = X[:, i]\n",
        "    sample = np.zeros((5, 1))\n",
        "    # Every 5 datapoints in the column is one column in the sample\n",
        "    bar_data = []\n",
        "    for j in range(len(col)):\n",
        "      bar_data.append(col[j])\n",
        "      if (j + 1) % 5 == 0:\n",
        "        sample = np.concatenate((sample, np.transpose([np.array(bar_data)])), axis=1)\n",
        "        bar_data = []\n",
        "    sample = np.delete(sample, 0, axis=1)\n",
        "    new_X.append(sample)\n",
        "  return np.array(new_X)"
      ],
      "metadata": {
        "id": "7An3FqMpAZjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = format_rnn(X_train, 15)\n",
        "X_test = format_rnn(X_test, 15)\n",
        "Y_train = Y_train.flatten()\n",
        "Y_test = Y_test.flatten()"
      ],
      "metadata": {
        "id": "YdFHuL_hBktX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.layers import Reshape\n",
        "from keras.initializers import glorot_normal\n",
        "\n",
        "def single_layer_rnn(input_shape = (5, 15)):\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  X_input = Input(input_shape)\n",
        "  X = SimpleRNN(1, activation='relu', kernel_initializer=glorot_normal())(X_input)\n",
        "  model = Model(inputs = X_input, outputs = X, name='single layer rnn')\n",
        "  return model\n",
        "\n",
        "model = single_layer_rnn()\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PysWBzqSAoYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 200, batch_size = 32)"
      ],
      "metadata": {
        "id": "wqzBwjYlBVv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two Layer RNN"
      ],
      "metadata": {
        "id": "g0JOIYEjqs5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.layers import Reshape\n",
        "from keras.initializers import glorot_normal\n",
        "\n",
        "def two_layer_rnn(input_shape = (5, 15)):\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  X_input = Input(input_shape)\n",
        "  X = SimpleRNN(4, activation='relu', return_sequences=True, kernel_initializer=glorot_normal())(X_input)\n",
        "  X = SimpleRNN(1, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  model = Model(inputs = X_input, outputs = X, name='single layer rnn')\n",
        "  return model\n",
        "\n",
        "model = two_layer_rnn()\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "D8tMT0yiDlAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 200, batch_size = 32)"
      ],
      "metadata": {
        "id": "v7uUODQpEWtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "L4Ku7hEq-LSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv1D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, RNN\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Model, load_model\n",
        "from keras.initializers import glorot_uniform, glorot_normal\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "metadata": {
        "id": "eNHUWu9EYk9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format X data to fit in CNN\n",
        "# Format: m sets of 5xdx1 matricies, where d is the number of candles in the window\n",
        "def format(X, d):\n",
        "  # Each window is a column of X\n",
        "  new_X = []\n",
        "  for i in range(len(X[0])):\n",
        "    col = X[:, i]\n",
        "    sample = np.zeros((5, 1))\n",
        "    # Every 5 datapoints in the column is one column in the sample\n",
        "    bar_data = []\n",
        "    for j in range(len(col)):\n",
        "      bar_data.append(col[j])\n",
        "      if (j + 1) % 5 == 0:\n",
        "        sample = np.concatenate((sample, np.transpose([np.array(bar_data)])), axis=1)\n",
        "        bar_data = []\n",
        "    sample = np.delete(sample, 0, axis=1)\n",
        "    sample = np.expand_dims(sample, 2)\n",
        "    new_X.append(sample)\n",
        "  return np.array(new_X)"
      ],
      "metadata": {
        "id": "eWYnkLIUAjsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = format(X_train, 10)\n",
        "print(X_train.shape)\n",
        "Y_train = np.squeeze(Y_train)"
      ],
      "metadata": {
        "id": "FeQnEeFN3Yan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.convolutional import Conv3D\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Reshape\n",
        "\n",
        "def basic_cnn(input_shape = (5, 10, 1)):\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  X_input = Input(input_shape)\n",
        "  X = Conv2D(filters=5, kernel_size=(3, 3), padding='valid', kernel_initializer=glorot_normal())(X_input)\n",
        "  X = Conv2D(filters=5, kernel_size=(3, 3), padding='valid', kernel_initializer=glorot_normal())(X_input)\n",
        "  X = Flatten()(X)\n",
        "  X = Dense(128, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(64, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(32, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(16, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(1, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  model = Model(inputs = X_input, outputs = X, name='BasicCNN')\n",
        "  return model"
      ],
      "metadata": {
        "id": "CT8Cf3ZZ-bm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = format(X_test, 10)\n",
        "Y_test = np.squeeze(Y_test)"
      ],
      "metadata": {
        "id": "FgB7bvX6QLLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = basic_cnn()\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7ee1BEctXNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 200, batch_size = 32)"
      ],
      "metadata": {
        "id": "_FbTGkoVsKg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.evaluate(X_test, Y_test)\n",
        "predictions = model.predict(X_test)\n",
        "print('Average Error', preds[0])\n",
        "\n",
        "# Convert prediction back into actual prediction\n",
        "mre = 0\n",
        "pred_range = len(predictions)\n",
        "for i in range(pred_range):\n",
        "  maxprice = 450.094\n",
        "  minprice = 52.0695\n",
        "  meanprice = 179.1915384746307\n",
        "  stdprice = 101.32692381699914\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_price = Y_test[i]*(maxprice - minprice) + minprice\n",
        "  #actual_predict = predictions[i]*stdprice + meanprice\n",
        "  #true_price = Y_test[i]*stdprice + meanprice\n",
        "  mre += abs(actual_predict - true_price)/(true_price)\n",
        "  #print(actual_predict, true_price)\n",
        "print('MRE on Actual Prices:', mre[0]/pred_range)"
      ],
      "metadata": {
        "id": "c9I3_mTNP6_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-RNN"
      ],
      "metadata": {
        "id": "JqcQs0zv3rmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, LSTM, Flatten, TimeDistributed, Conv2D, Reshape, SimpleRNN, GRU\n",
        "from keras import Sequential\n",
        "from keras.initializers import glorot_normal, glorot_uniform, HeNormal, HeUniform\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=3, kernel_size=(2, 3), strides=1, padding='valid', kernel_initializer=glorot_normal(), input_shape=(5, 10, 1)))\n",
        "model.add(Conv2D(filters=2, kernel_size=(2, 3), strides=1, padding='valid', kernel_initializer=glorot_normal()))\n",
        "model.add(Reshape((6, 6)))\n",
        "model.add(SimpleRNN(20, activation='relu', kernel_initializer=glorot_normal()))\n",
        "model.add(Dense(1, activation='relu', kernel_initializer=glorot_normal()))\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RS4SLpAP7G6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = format(X_train, 10)\n",
        "Y_train = np.squeeze(Y_train)\n",
        "X_test = format(X_test, 10)\n",
        "Y_test = np.squeeze(Y_test)"
      ],
      "metadata": {
        "id": "RimOkgDKAfF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 200, batch_size = 32)"
      ],
      "metadata": {
        "id": "rcErOAQW8i8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.evaluate(X_test, Y_test)\n",
        "predictions = model.predict(X_test)\n",
        "print('Mean Error', preds)\n",
        "\n",
        "# Convert prediction back into actual prediction\n",
        "mre = 0\n",
        "overpredict = 0\n",
        "for i in range(len(predictions)):\n",
        "  maxprice = 450.094\n",
        "  minprice = 52.0695\n",
        "  meanprice = 179.1915384746307\n",
        "  stdprice = 101.09464575990093\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_price = Y_test[i]*(maxprice - minprice) + minprice\n",
        "  #actual_predict = np.exp(predictions[i])\n",
        "  #true_price = np.exp(Y_test[i])\n",
        "  \n",
        "  #actual_predict = predictions[i]*stdprice + meanprice\n",
        "  #true_price = Y_test[i]*stdprice + meanprice\n",
        "  if actual_predict > true_price:\n",
        "    overpredict += 1\n",
        "  mre += abs(actual_predict - true_price)/(true_price)\n",
        "  #print(actual_predict, true_price)\n",
        "print('MRE on Actual Prices:', mre[0]/len(predictions))\n",
        "print('Overpredict Rate', overpredict/len(predictions))"
      ],
      "metadata": {
        "id": "QeH0-OZo9rnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulation: buy a share at projected low if price reaches the projected low, then sell at close\n",
        "output_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinputandoutput.csv', header=None))\n",
        "removed = 0\n",
        "output_data = output_data[15:]\n",
        "output_data = output_data[66000:]\n",
        "#print(len(output_data), len(predictions))\n",
        "print('For the test interval, the price of SPY changed from', output_data[0][4], 'to', output_data[len(output_data)-1][4])\n",
        "\n",
        "maxprice = 450.094\n",
        "minprice = 52.0695\n",
        "acc_value = 1000\n",
        "winners = 0\n",
        "buys = 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_low = output_data[i][3]\n",
        "  true_close = output_data[i][4]\n",
        "  #print(actual_predict, true_low)\n",
        "  # 0.9942 is a \"correction\" constant to account for the model's error\n",
        "  if true_low < actual_predict*0.9942:\n",
        "    acc_value = acc_value - actual_predict*0.9942+ true_close\n",
        "    if actual_predict*0.9942 <= true_close:\n",
        "      winners += 1\n",
        "    buys += 1\n",
        "    #print('Bought at:', str(actual_predict*0.9942, ' Sold at:', str(true_close), ' Current Acc Value:', str(acc_value))\n",
        "print('This current strategy produces', str((acc_value[0]/1000 - 1)*100)+ \"% Profit over the test timeframe\")\n",
        "print('Winrate:', winners/buys)\n",
        "print('Total Transactions:', buys)"
      ],
      "metadata": {
        "id": "Qcu-54aMrBOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}