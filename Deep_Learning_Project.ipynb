{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FRLcfpyoixTP",
        "M8gPJDKE9gg2",
        "g0JOIYEjqs5V",
        "L4Ku7hEq-LSS",
        "JqcQs0zv3rmW"
      ],
      "toc_visible": true,
      "mount_file_id": "19jTFkWDq4eQdxbOdg_mo8nvlJfBpBT6_",
      "authorship_tag": "ABX9TyNqNWZ7A4VjqDKzqbNKU0VS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronlwan/deep-learning-stock-lows/blob/main/Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVJLbCZPkkK-"
      },
      "outputs": [],
      "source": [
        "# Import Modules\n",
        "!pip install yfinance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import math\n",
        "from tensorflow.python.framework import ops\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "765D6wYFbq-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data"
      ],
      "metadata": {
        "id": "0j5ab1cNjhkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Data Within Window (min-max)\n",
        "def normalize_in_window(data):\n",
        "  new_data = []\n",
        "  for j in range(len(data)):\n",
        "    prices = []\n",
        "    volumes = []\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1) % 5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "    maxprice = max(prices)\n",
        "    minprice = min(prices)\n",
        "    maxvol = max(volumes)\n",
        "    minvol = min(volumes)\n",
        "    newrow = []\n",
        "    price_index = 0\n",
        "    vol_index = 0\n",
        "    for k in range(len(data[j])):\n",
        "      if (k + 1) % 5 == 0:\n",
        "        newrow.append((volumes[vol_index] - minvol)/(maxvol-minvol))\n",
        "        vol_index += 1\n",
        "      else:\n",
        "        newrow.append((prices[price_index] - minprice)/(maxprice - minprice))\n",
        "        price_index += 1\n",
        "    newrow.append(maxprice)\n",
        "    newrow.append(minprice)\n",
        "    new_data.append(newrow)\n",
        "  data = np.array(new_data)\n",
        "  return data\n",
        "\n",
        "# Normalize Over the Training Timeframe (min-max)\n",
        "def normalize_all(data, train_window):\n",
        "  prices = []\n",
        "  volumes = []\n",
        "  for j in range(train_window):\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "  maxprice = max(prices)\n",
        "  minprice = min(prices)\n",
        "  minvol = min(volumes)\n",
        "  maxvol = max(volumes)\n",
        "  print('Max Price:', maxprice)\n",
        "  print('Min Price:', minprice)\n",
        "  new_data = []\n",
        "  for k in range(len(data)):\n",
        "    row = []\n",
        "    for i in range(len(data[k])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        row.append((data[k][i]-minvol)/(maxvol-minvol))\n",
        "      else:\n",
        "        row.append((data[k][i]-minprice)/(maxprice-minprice))\n",
        "    new_data.append(row)\n",
        "  return np.array(new_data)\n",
        "\n",
        "# Standardize over the whole Training Timeframe (0 mean, 1std)\n",
        "def standardize_all(data):\n",
        "  prices = []\n",
        "  volumes = []\n",
        "  for j in range(len(data)):\n",
        "    for i in range(len(data[j])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        volumes.append(data[j][i])\n",
        "      else:\n",
        "        prices.append(data[j][i])\n",
        "  meanprice = np.average(np.array(prices))\n",
        "  stdprice = np.std(np.array(prices))\n",
        "  meanvol = np.sum(np.array(volumes))/len(prices)\n",
        "  stdvol = np.std(np.array(volumes))\n",
        "  print('Mean Price:', meanprice)\n",
        "  print('STD Price:', stdprice)\n",
        "  new_data = []\n",
        "  for k in range(len(data)):\n",
        "    row = []\n",
        "    for i in range(len(data[k])):\n",
        "      if (i + 1)%5 == 0:\n",
        "        row.append((data[k][i]-meanvol)/(stdvol))\n",
        "      else:\n",
        "        row.append((data[k][i]-meanprice)/(stdprice))\n",
        "    new_data.append(row)\n",
        "  return np.array(new_data)\n",
        "\n",
        "def log_normalization(data):\n",
        "  return np.log(data)"
      ],
      "metadata": {
        "id": "5zYVw9RMtwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### yfinance Data Processsing"
      ],
      "metadata": {
        "id": "mEMYBWjijVrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data From Yfinance\n",
        "def getData(ticker, input_interval, input_candles, output_interval):\n",
        "\n",
        "  # Load Input Data\n",
        "  # Input Data Columns: Date, Open, High, Low, Close, Volume\n",
        "  input = yf.download(ticker, interval=input_interval)\n",
        "  input = np.array(pd.DataFrame({'Date': input.index, 'Open': input.values[:, 0], 'High': input.values[:, 1],\n",
        "                         'Low': input.values[:, 2], 'Close': input.values[:, 3],\n",
        "                         'Volume': input.values[:, 5]}))\n",
        "  # Output Data Colums: Date, Low\n",
        "  # Load Output Data\n",
        "  output = yf.download(ticker, interval=output_interval)\n",
        "  output = np.array(pd.DataFrame({'Date': output.index, 'Low': output.values[:, 2]}))\n",
        "  \n",
        "  # Match Input w/ Output\n",
        "  # Matched Data Columns: Repeat (Date, Open, High, Low, Close, Volume) for each input candle, Output Low\n",
        "  data = []\n",
        "  input_dates = list(input[:, 0])\n",
        "  for row in output:\n",
        "    data_row = []\n",
        "    outdate = row[0]\n",
        "    try:\n",
        "      input_index = input_dates.index(outdate)\n",
        "      # Collect the input_candles preceeding the outdate\n",
        "      for i in range(input_candles):\n",
        "        candle = input[input_index - 1 - i][1:]\n",
        "        for item in candle:\n",
        "          data_row.append(item)\n",
        "      data_row.append(row[1])\n",
        "      data.append(data_row)\n",
        "    except: \n",
        "      pass\n",
        "  return np.array(data)"
      ],
      "metadata": {
        "id": "8uxA_LK6wHSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data\n",
        "data = getData(\"SPY\", \"1d\", 10, \"1d\")\n",
        "print(data.shape)\n",
        "data = normalize_all(data, 7000)"
      ],
      "metadata": {
        "id": "Ug3supUgttdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intraday Data Processing"
      ],
      "metadata": {
        "id": "DzgorHNniX8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intraday Process (5min to Predict 1hr)\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/5min.csv', header=None))\n",
        "output_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hr.csv', header=None))\n",
        "data = []\n",
        "deleted = []\n",
        "i = 0\n",
        "output_data = output_data[1:]\n",
        "index = 0\n",
        "while True:\n",
        "  try:\n",
        "    # Select 24 candles of data\n",
        "    selection = input_data[i: i+24][:, 1:]\n",
        "    last_timestamp = input_data[i: i+24][:, 0][23]\n",
        "    # Check last candle, if it doesn't match up we just omit from dataset b/c we have a lot of datapoints\n",
        "    if int(last_timestamp[14:16]) == 55:\n",
        "      selection = list(selection.flatten())\n",
        "      selection.append(output_data[index][1])\n",
        "      data.append(selection)\n",
        "      i += 12\n",
        "    else:\n",
        "      deleted.append(index)\n",
        "      difference = int(last_timestamp[14:16]) + 60 - 55\n",
        "      i += int(12 - difference/5)\n",
        "    index += 1\n",
        "  except:\n",
        "    break\n",
        "\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "print(deleted)\n",
        "data = normalize_all(data, 33000)"
      ],
      "metadata": {
        "id": "S5gNGL7icrih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1hr to predict 1d\n",
        "from pandas import Timestamp\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinput.csv', header=None))\n",
        "output = yf.download('SPY', interval='1d')\n",
        "output_data = np.array(pd.DataFrame({'Date': output.index, 'Low': output.values[:, 2]}))\n",
        "outdates = list(output_data[:, 0])\n",
        "start = outdates.index(Timestamp('2005-01-05 00:00:00'))\n",
        "output_data = output_data[start:]\n",
        "data = []\n",
        "deleted = []\n",
        "i = 0\n",
        "index = 0\n",
        "while True:\n",
        "  try:\n",
        "    # Select 24 candles of data\n",
        "    selection = input_data[i: i+24][:, 1:]\n",
        "    last_timestamp = input_data[i: i+24][:, 0][23]\n",
        "    # Check last candle, if it doesn't match up we just omit from dataset b/c we have a lot of datapoints\n",
        "    if int(last_timestamp[11:13]) == 19:\n",
        "      selection = list(selection.flatten())\n",
        "      selection.append(output_data[index][1])\n",
        "      data.append(selection)\n",
        "      i += 12\n",
        "    else:\n",
        "      deleted.append(index)\n",
        "      difference = int(last_timestamp[11:13]) - 7\n",
        "      i += int(12 - difference)\n",
        "    index += 1\n",
        "  except:\n",
        "    break\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "data = log_normalization(data)"
      ],
      "metadata": {
        "id": "w9eJrL2-1V7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1hr to predict 1hr\n",
        "input_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinputandoutput.csv', header=None))\n",
        "output_data = input_data[:, 3]\n",
        "data = []\n",
        "index = 0\n",
        "output_data = output_data[10:]\n",
        "for i in range(len(output_data)):\n",
        "  row = input_data[i: i+10, 1:]\n",
        "  row = row.flatten()\n",
        "  row = list(row)\n",
        "  row.append(output_data[i])\n",
        "  data.append(row)\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "data = normalize_all(data, 66000)"
      ],
      "metadata": {
        "id": "Kz6FLXvJn0T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data"
      ],
      "metadata": {
        "id": "FRLcfpyoixTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Train/Test based on time frame (simulates predicting the future using past data)\n",
        "x = data[:, [i for i in range(50)]]\n",
        "y = np.transpose([data[:, 50]])\n",
        "X_train, X_test, Y_train, Y_test = np.transpose(x[:66000]), np.transpose(x[66000:]), np.transpose(y[:66000]), np.transpose(y[66000:])"
      ],
      "metadata": {
        "id": "piCaaerZcoBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Layer RNN"
      ],
      "metadata": {
        "id": "M8gPJDKE9gg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Mini-Batches\n",
        "def random_mini_batches(X, Y, mini_batch_size=64):\n",
        "  mini_batches = []\n",
        "  m = X.shape[1]\n",
        "  num_complete_mini_batches = math.floor(m/mini_batch_size)\n",
        "  for k in range(num_complete_mini_batches):\n",
        "    batch_X = X[:, k*mini_batch_size: (k+1)*mini_batch_size]\n",
        "    batch_Y = Y[:, k*mini_batch_size: (k+1)*mini_batch_size]\n",
        "    mini_batches.append((batch_X, batch_Y))\n",
        "  if m % mini_batch_size != 0:\n",
        "    batch_X = X[:, mini_batch_size * num_complete_mini_batches:]\n",
        "    batch_Y = Y[:, mini_batch_size * num_complete_mini_batches:]\n",
        "    mini_batches.append((batch_X, batch_Y))\n",
        "  return mini_batches"
      ],
      "metadata": {
        "id": "GINiSpq8rk_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y):\n",
        "  X = tf.placeholder(tf.float32, shape=[n_x, None], name='Placeholder_1')\n",
        "  Y = tf.placeholder(tf.float32, shape=[n_y, None], name='Placeholder_2')\n",
        "  h_prev = tf.placeholder(tf.float32, [8, 1], name='h_prev')\n",
        "  return X, Y, h_prev\n"
      ],
      "metadata": {
        "id": "bBossYab9cYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, h_prev = create_placeholders(8, 1)\n",
        "print (\"X = \" + str(X))\n",
        "print (\"Y = \" + str(Y))\n",
        "print(h_prev)"
      ],
      "metadata": {
        "id": "P9F_CJWI9z0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf2\n",
        "\n",
        "def initialize_parameters(parameter_size, hidden_size):\n",
        "  Wa = tf.get_variable(\"Wa\", [hidden_size, parameter_size], initializer = tf2.initializers.GlorotNormal())\n",
        "  Wya = tf.get_variable(\"Wya\", [1, hidden_size], initializer = tf2.initializers.GlorotNormal())\n",
        "  Waa = tf.get_variable(\"Waa\", [hidden_size, hidden_size], initializer = tf2.initializers.GlorotNormal())\n",
        "\n",
        " \n",
        "\n",
        "  ba = tf.get_variable(\"ba\", [hidden_size, 1], initializer=tf.zeros_initializer())\n",
        "  by = tf.get_variable(\"by\", [1, 1], initializer=tf.zeros_initializer())\n",
        "  parameters = {\n",
        "      \"Wa\": Wa,\n",
        "      \"Wya\": Wya,\n",
        "      \"Waa\": Waa,\n",
        "      \"ba\": ba,\n",
        "      \"by\": by,\n",
        "  }\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "Q5BAmVxt97nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters, h_prev, max_batch):\n",
        "  Wa = parameters[\"Wa\"]\n",
        "  Wya = parameters[\"Wya\"]\n",
        "  Waa = parameters[\"Waa\"]\n",
        "  ba = parameters[\"ba\"]\n",
        "  by = parameters[\"by\"]\n",
        " \n",
        "  a, y = {}, []\n",
        "  a[-1] = h_prev\n",
        "  for t in range(max_batch):\n",
        "    try:\n",
        "      a[t] = tf.nn.relu(tf.add(tf.add(ba, tf.matmul(Waa, a[t - 1])), tf.matmul(Wa, X[:, t:t+1])))\n",
        "      yt = tf.nn.relu(tf.add(tf.matmul(Wya, a[t]), by))\n",
        "      y.append(yt)\n",
        "    except:\n",
        "      pass\n",
        "  return y, a"
      ],
      "metadata": {
        "id": "iWIkORXHBxho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(predictions, Y):\n",
        "  cost = tf.reduce_mean(tf.math.abs(tf.subtract(predictions, Y)))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "9dtl8jJsLpcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 500, print_cost = True, minibatch_size=16):\n",
        "  ops.reset_default_graph()\n",
        "  (n_x, m) = X_train.shape\n",
        "  n_y = Y_train.shape[0]\n",
        "  costs = []\n",
        "  \n",
        "  X, Y, h_prev = create_placeholders(n_x, n_y)\n",
        "  parameters = initialize_parameters(n_x, 8)\n",
        "  preds, h = forward_propagation(X, parameters, h_prev, minibatch_size)\n",
        "  cost = compute_cost(preds, Y)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  print(\"Beginning Training\")\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "      hprevious = np.zeros((8, 1))\n",
        "      epoch_cost = 0                      # Defines a cost related to an epoch\n",
        "      num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "      minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
        "      for i in range(num_minibatches):\n",
        "          # Select a minibatch\n",
        "          (minibatch_X, minibatch_Y) = minibatches[i]\n",
        "          _, minibatch_cost, h_new = sess.run([optimizer, cost, h], feed_dict={X: minibatch_X, Y: minibatch_Y, h_prev: hprevious})\n",
        "          epoch_cost += minibatch_cost\n",
        "          hprevious = h_new[list(h.keys())[-1]]\n",
        "      epoch_cost/=num_minibatches\n",
        "\n",
        "      # Print the cost every epoch\n",
        "      if print_cost == True and epoch % 100 == 0:\n",
        "          print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "      if print_cost == True and epoch % 5 == 0:\n",
        "          costs.append(epoch_cost)\n",
        "\n",
        "    \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per fives)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    # Save parameters\n",
        "    parameters = sess.run(parameters)\n",
        "    print (\"Parameters have been trained!\")\n",
        "\n",
        "    \n",
        "    return parameters, hprevious"
      ],
      "metadata": {
        "id": "PEdI6DZuPl4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, h = model(X_train, Y_train, X_test, Y_test)"
      ],
      "metadata": {
        "id": "j2lMgyS6VvkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(X, Y, parameters, hprevious):\n",
        "  Wa = parameters[\"Wa\"]\n",
        "  Wya = parameters[\"Wya\"]\n",
        "  Waa = parameters[\"Waa\"]\n",
        "  ba = parameters[\"ba\"]\n",
        "  by = parameters[\"by\"]\n",
        "  a, y = {}, []\n",
        "  a[-1] = hprevious\n",
        "  for t in range(X.shape[1]):\n",
        "    a[t] = np.maximum(np.add(np.add(ba, np.matmul(Waa, a[t - 1])), np.matmul(Wa, X[:, t:t+1])), 0)\n",
        "    yt = np.maximum(np.add(np.matmul(Wya, a[t]), by), 0)\n",
        "    y.append(yt)\n",
        "\n",
        "\n",
        "  error = 0\n",
        "  for i in range(len(y)):\n",
        "    error += abs(y[i] - Y[0][i])\n",
        "  print('MAE on Normalized Data', error/len(y))\n",
        "\n",
        "\n",
        "  # Convert prediction back into actual prediction\n",
        "  mre = 0\n",
        "  for i in range(len(y)):\n",
        "    maxprice = 450.6300048828125\n",
        "    minprice = 42.8125\n",
        "    actual_predict = y[i]*(maxprice - minprice) + minprice\n",
        "    true_price = Y[0][i]*(maxprice - minprice) + minprice\n",
        "    #print(actual_predict, true_price)\n",
        "    mre += abs(actual_predict - true_price)/(true_price)\n",
        "  print('MRE on Actual Prices:', mre[0][0]/len(y))\n",
        "\n",
        "compute_accuracy(X_test, Y_test, parameters, h)"
      ],
      "metadata": {
        "id": "H_VmjkZtKVS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two Layer RNN"
      ],
      "metadata": {
        "id": "g0JOIYEjqs5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y):\n",
        "  X = tf.placeholder(tf.float32, shape=[n_x, None], name='Placeholder_1')\n",
        "  Y = tf.placeholder(tf.float32, shape=[n_y, None], name='Placeholder_2')\n",
        "  h_prev = tf.placeholder(tf.float32, [8, 1], name='h_prev')\n",
        "  h2_prev = tf.placeholder(tf.float32, [4, 1], name='h2_prev')\n",
        "  return X, Y, h_prev, h2_prev\n"
      ],
      "metadata": {
        "id": "d4wF0_MWqvSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, h_prev, h2_prev = create_placeholders(4, 1)\n",
        "print (\"X = \" + str(X))\n",
        "print (\"Y = \" + str(Y))\n",
        "print(h_prev)\n",
        "print(h2_prev)"
      ],
      "metadata": {
        "id": "JQ4Lw5AiqvSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf2\n",
        "\n",
        "def initialize_parameters(parameter_size, hidden_size):\n",
        "  Wa = tf.get_variable(\"Wa\", [hidden_size, parameter_size], initializer = tf2.initializers.GlorotNormal())\n",
        "  Wya = tf.get_variable(\"Wya\", [hidden_size, hidden_size], initializer = tf2.initializers.GlorotNormal())\n",
        "  Waa = tf.get_variable(\"Waa\", [hidden_size, hidden_size], initializer = tf2.initializers.GlorotNormal())\n",
        "\n",
        "  Wa2 = tf.get_variable(\"Wa2\", [4, hidden_size], initializer = tf2.initializers.GlorotNormal())\n",
        "  Wya2 = tf.get_variable(\"Wya2\", [1, 4], initializer = tf2.initializers.GlorotNormal())\n",
        "  Waa2 = tf.get_variable(\"Waa2\", [4, 4], initializer = tf2.initializers.GlorotNormal())\n",
        "\n",
        "  ba = tf.get_variable(\"ba\", [hidden_size, 1], initializer=tf.zeros_initializer())\n",
        "  ba2 = tf.get_variable(\"ba2\", [4, 1], initializer=tf.zeros_initializer())\n",
        "  by = tf.get_variable(\"by\", [hidden_size, 1], initializer=tf.zeros_initializer())\n",
        "  by2 = tf.get_variable(\"by2\", [1, 1], initializer=tf.zeros_initializer())\n",
        "  parameters = {\n",
        "      \"Wa\": Wa,\n",
        "      \"Wya\": Wya,\n",
        "      \"Waa\": Waa,\n",
        "      \"ba\": ba,\n",
        "      \"by\": by,\n",
        "      \"Wa2\": Wa2,\n",
        "      \"Wya2\": Wya2,\n",
        "      \"Waa2\": Waa2,\n",
        "      \"ba2\": ba2,\n",
        "      \"by2\": by2\n",
        "  }\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "G-i3lp-FqvSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters, h_prev, h2_prev, max_batch):\n",
        "  Wa = parameters[\"Wa\"]\n",
        "  Wya = parameters[\"Wya\"]\n",
        "  Waa = parameters[\"Waa\"]\n",
        "  ba = parameters[\"ba\"]\n",
        "  by = parameters[\"by\"]\n",
        "  Wa2 = parameters[\"Wa2\"]\n",
        "  Wya2 = parameters[\"Wya2\"]\n",
        "  Waa2 = parameters[\"Waa2\"]\n",
        "  ba2 = parameters[\"ba2\"]\n",
        "  by2 = parameters[\"by2\"]\n",
        "  a, a2, y = {}, {}, []\n",
        "  a[-1] = h_prev\n",
        "  a2[-1] = h2_prev\n",
        "  for t in range(max_batch):\n",
        "    try:\n",
        "      a[t] = tf.nn.relu(tf.add(tf.add(ba, tf.matmul(Waa, a[t - 1])), tf.matmul(Wa, X[:, t:t+1])))\n",
        "      yt = tf.nn.relu(tf.add(tf.matmul(Wya, a[t]), by))\n",
        "      a2[t] = tf.nn.relu(tf.add(tf.add(ba2, tf.matmul(Waa2, a2[t - 1])), tf.matmul(Wa2, yt)))\n",
        "      yt2 = tf.nn.relu(tf.add(tf.matmul(Wya2, a2[t]), by2))\n",
        "      y.append(yt2)\n",
        "    except:\n",
        "      pass\n",
        "  return y, a, a2"
      ],
      "metadata": {
        "id": "aJEfqps1qvSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(predictions, Y):\n",
        "  cost = tf.reduce_mean(tf.math.abs(tf.subtract(predictions, Y)))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "ppzsNAyzqvSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1000, print_cost = True, minibatch_size=8):\n",
        "  ops.reset_default_graph()\n",
        "  (n_x, m) = X_train.shape\n",
        "  n_y = Y_train.shape[0]\n",
        "  costs = []\n",
        "  \n",
        "  X, Y, h_prev, h2_prev = create_placeholders(n_x, n_y)\n",
        "  parameters = initialize_parameters(n_x, 8)\n",
        "  preds, h, h2 = forward_propagation(X, parameters, h_prev, h2_prev, minibatch_size)\n",
        "  cost = compute_cost(preds, Y)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  print(\"Beginning Training\")\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "      hprevious = np.zeros((8, 1))\n",
        "      h2previous = np.zeros((4, 1))\n",
        "      epoch_cost = 0                      # Defines a cost related to an epoch\n",
        "      num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "      minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
        "      for i in range(num_minibatches):\n",
        "          # Select a minibatch\n",
        "          (minibatch_X, minibatch_Y) = minibatches[i]\n",
        "          _, minibatch_cost, h_new, h2_new = sess.run([optimizer, cost, h, h2], feed_dict={X: minibatch_X, Y: minibatch_Y, h_prev: hprevious, h2_prev: h2previous})\n",
        "          epoch_cost += minibatch_cost\n",
        "          hprevious = h_new[list(h.keys())[-1]]\n",
        "          h2previous = h2_new[list(h.keys())[-1]]\n",
        "      epoch_cost/=num_minibatches\n",
        "\n",
        "      # Print the cost every epoch\n",
        "      if print_cost == True and epoch % 100 == 0:\n",
        "          print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "      if print_cost == True and epoch % 5 == 0:\n",
        "          costs.append(epoch_cost)\n",
        "\n",
        "    \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per fives)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    # Save parameters\n",
        "    parameters = sess.run(parameters)\n",
        "    print (\"Parameters have been trained!\")\n",
        "\n",
        "    \n",
        "    \n",
        "    return parameters, hprevious, h2previous"
      ],
      "metadata": {
        "id": "gm2Xc5QJqvSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, h, h2 = model(X_train, Y_train, X_test, Y_test)"
      ],
      "metadata": {
        "id": "rv-ynd4JqvSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(X, Y, parameters, hprevious, h2previous):\n",
        "  Wa = parameters[\"Wa\"]\n",
        "  Wya = parameters[\"Wya\"]\n",
        "  Waa = parameters[\"Waa\"]\n",
        "  ba = parameters[\"ba\"]\n",
        "  by = parameters[\"by\"]\n",
        "  Wa2 = parameters[\"Wa2\"]\n",
        "  Wya2 = parameters[\"Wya2\"]\n",
        "  Waa2 = parameters[\"Waa2\"]\n",
        "  ba2 = parameters[\"ba2\"]\n",
        "  by2 = parameters[\"by2\"]\n",
        "  a, a2, y = {}, {}, []\n",
        "  a[-1] = hprevious\n",
        "  a2[-1] = h2previous\n",
        "  for t in range(X.shape[1]):\n",
        "    a[t] = np.maximum(np.add(np.add(ba, np.matmul(Waa, a[t - 1])), np.matmul(Wa, X[:, t:t+1])), 0)\n",
        "    yt = np.maximum(np.add(np.matmul(Wya, a[t]), by), 0)\n",
        "    a2[t] = np.maximum(np.add(np.add(ba2, np.matmul(Waa2, a2[t - 1])), np.matmul(Wa2, yt)), 0)\n",
        "    yt2 = np.maximum(np.add(np.matmul(Wya2, a2[t]), by2), 0)\n",
        "    y.append(yt2)\n",
        "\n",
        "\n",
        "  error = 0\n",
        "  for i in range(len(y)):\n",
        "    error += abs(y[i] - Y[0][i])\n",
        "  print('MAE on Normalized Data', error/len(y))\n",
        "\n",
        "\n",
        "  # Convert prediction back into actual prediction\n",
        "  mre = 0\n",
        "  for i in range(len(y)):\n",
        "    maxprice = 429.6400146484375\n",
        "    minprice = 42.8125\n",
        "    actual_predict = y[i]*(maxprice - minprice) + minprice\n",
        "    true_price = Y[0][i]*(maxprice - minprice) + minprice\n",
        "    #print(actual_predict, true_price)\n",
        "    mre += abs(actual_predict - true_price)/(true_price)\n",
        "  print('MRE on Actual Prices:', mre[0][0]/len(y))\n",
        "\n",
        "compute_accuracy(X_test, Y_test, parameters, h, h2)"
      ],
      "metadata": {
        "id": "nkFv-X5BqvSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "L4Ku7hEq-LSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv1D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, RNN\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Model, load_model\n",
        "from keras.initializers import glorot_uniform, glorot_normal\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "metadata": {
        "id": "eNHUWu9EYk9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format X data to fit in CNN\n",
        "# Format: m sets of 5xdx1 matricies, where d is the number of candles in the window\n",
        "def format(X, d):\n",
        "  # Each window is a column of X\n",
        "  new_X = []\n",
        "  for i in range(len(X[0])):\n",
        "    col = X[:, i]\n",
        "    sample = np.zeros((5, 1))\n",
        "    # Every 5 datapoints in the column is one column in the sample\n",
        "    bar_data = []\n",
        "    for j in range(len(col)):\n",
        "      bar_data.append(col[j])\n",
        "      if (j + 1) % 5 == 0:\n",
        "        sample = np.concatenate((sample, np.transpose([np.array(bar_data)])), axis=1)\n",
        "        bar_data = []\n",
        "    sample = np.delete(sample, 0, axis=1)\n",
        "    sample = np.expand_dims(sample, 2)\n",
        "    new_X.append(sample)\n",
        "  return np.array(new_X)\n",
        "X_train = format(X_train, 10)\n",
        "print(X_train.shape)\n",
        "Y_train = np.squeeze(Y_train)"
      ],
      "metadata": {
        "id": "eWYnkLIUAjsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.convolutional import Conv3D\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Reshape\n",
        "\n",
        "def basic_cnn(input_shape = (5, 10, 1)):\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  X_input = Input(input_shape)\n",
        "  X = Conv2D(filters=5, kernel_size=(3, 3), padding='valid', kernel_initializer=glorot_normal())(X_input)\n",
        "  X = Conv2D(filters=5, kernel_size=(3, 3), padding='valid', kernel_initializer=glorot_normal())(X_input)\n",
        "  X = Flatten()(X)\n",
        "  X = Dense(128, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(64, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(32, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(16, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  X = Dense(1, activation='relu', kernel_initializer=glorot_normal())(X)\n",
        "  model = Model(inputs = X_input, outputs = X, name='BasicCNN')\n",
        "  return model"
      ],
      "metadata": {
        "id": "CT8Cf3ZZ-bm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = format(X_test, 10)\n",
        "Y_test = np.squeeze(Y_test)"
      ],
      "metadata": {
        "id": "FgB7bvX6QLLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = basic_cnn()\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7ee1BEctXNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 100, batch_size = 32)"
      ],
      "metadata": {
        "id": "_FbTGkoVsKg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.evaluate(X_test, Y_test)\n",
        "predictions = model.predict(X_test)\n",
        "print('Average Error', preds[0])\n",
        "\n",
        "# Convert prediction back into actual prediction\n",
        "mre = 0\n",
        "pred_range = len(predictions)\n",
        "for i in range(pred_range):\n",
        "  maxprice = 450.094\n",
        "  minprice = 52.0695\n",
        "  meanprice = 179.1915384746307\n",
        "  stdprice = 101.32692381699914\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_price = Y_test[i]*(maxprice - minprice) + minprice\n",
        "  #actual_predict = predictions[i]*stdprice + meanprice\n",
        "  #true_price = Y_test[i]*stdprice + meanprice\n",
        "  mre += abs(actual_predict - true_price)/(true_price)\n",
        "  #print(actual_predict, true_price)\n",
        "print('MRE on Actual Prices:', mre[0]/pred_range)"
      ],
      "metadata": {
        "id": "c9I3_mTNP6_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-RNN"
      ],
      "metadata": {
        "id": "JqcQs0zv3rmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want X to have shape m x 5 x d x 1\n",
        "# where m is total samples, d is number of days in each window, and 5 is the \n",
        "# number of features in each day\n",
        "def cnn_rnn_format_x(X, d):\n",
        "  # Each window is a column of X\n",
        "  new_X = []\n",
        "  for i in range(len(X[0])):\n",
        "    col = X[:, i]\n",
        "    sample = np.zeros((5, 1))\n",
        "    # Every 5 datapoints in the column is one column in the sample\n",
        "    bar_data = []\n",
        "    for j in range(len(col)):\n",
        "      bar_data.append(col[j])\n",
        "      if (j + 1) % 5 == 0:\n",
        "        sample = np.concatenate((sample, np.transpose([np.array(bar_data)])), axis=1)\n",
        "        bar_data = []\n",
        "    sample = np.delete(sample, 0, axis=1)\n",
        "    sample = np.expand_dims(sample, 2)\n",
        "    new_X.append(sample)\n",
        "  return np.array(new_X)\n",
        "\n",
        "print(cnn_rnn_format_x(X_train, 10).shape)\n",
        "X_train = cnn_rnn_format_x(X_train, 10)"
      ],
      "metadata": {
        "id": "byZrq33i31Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, LSTM, Flatten, TimeDistributed, Conv2D, Reshape, SimpleRNN, GRU\n",
        "from keras import Sequential\n",
        "from keras.initializers import glorot_normal, glorot_uniform, HeNormal, HeUniform\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=3, kernel_size=(2, 3), strides=1, padding='valid', kernel_initializer=glorot_normal(), input_shape=(5, 10, 1)))\n",
        "model.add(Conv2D(filters=2, kernel_size=(2, 3), strides=1, padding='valid', kernel_initializer=glorot_normal()))\n",
        "model.add(Reshape((4, 9)))\n",
        "model.add(SimpleRNN(20, activation='relu', kernel_initializer=glorot_normal()))\n",
        "model.add(Dense(1, activation='relu', kernel_initializer=glorot_normal()))\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RS4SLpAP7G6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = np.squeeze(Y_train)\n",
        "X_test = cnn_rnn_format_x(X_test, 10)\n",
        "Y_test = np.squeeze(Y_test)"
      ],
      "metadata": {
        "id": "RimOkgDKAfF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 100, batch_size = 32)"
      ],
      "metadata": {
        "id": "rcErOAQW8i8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.evaluate(X_test, Y_test)\n",
        "predictions = model.predict(X_test)\n",
        "print('Mean Error', preds)\n",
        "\n",
        "# Convert prediction back into actual prediction\n",
        "mre = 0\n",
        "overpredict = 0\n",
        "for i in range(len(predictions)):\n",
        "  maxprice = 450.094\n",
        "  minprice = 52.0695\n",
        "  meanprice = 179.1915384746307\n",
        "  stdprice = 101.09464575990093\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_price = Y_test[i]*(maxprice - minprice) + minprice\n",
        "  #actual_predict = np.exp(predictions[i])\n",
        "  #true_price = np.exp(Y_test[i])\n",
        "  \n",
        "  #actual_predict = predictions[i]*stdprice + meanprice\n",
        "  #true_price = Y_test[i]*stdprice + meanprice\n",
        "  if actual_predict > true_price:\n",
        "    overpredict += 1\n",
        "  mre += abs(actual_predict - true_price)/(true_price)\n",
        "  #print(actual_predict, true_price)\n",
        "print('MRE on Actual Prices:', mre[0]/len(predictions))\n",
        "print('Overpredict Rate', overpredict/len(predictions))"
      ],
      "metadata": {
        "id": "QeH0-OZo9rnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulation: buy a share at projected low if price reaches the projected low, then sell at close\n",
        "output_data = np.array(pd.read_csv('/content/drive/MyDrive/SPY_qjrt28/1hrinputandoutput.csv', header=None))\n",
        "removed = 0\n",
        "output_data = output_data[15:]\n",
        "output_data = output_data[66000:]\n",
        "#print(len(output_data), len(predictions))\n",
        "print('For the test interval, the price of SPY changed from', output_data[0][4], 'to', output_data[len(output_data)-1][4])\n",
        "\n",
        "maxprice = 450.094\n",
        "minprice = 52.0695\n",
        "acc_value = 1000\n",
        "winners = 0\n",
        "buys = 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  actual_predict = predictions[i]*(maxprice - minprice) + minprice\n",
        "  true_low = output_data[i][3]\n",
        "  true_close = output_data[i][4]\n",
        "  #print(actual_predict, true_low)\n",
        "  # 0.9942 is a \"correction\" constant to account for the model's error\n",
        "  if true_low < actual_predict*0.9942:\n",
        "    acc_value = acc_value - actual_predict*0.9942+ true_close\n",
        "    if actual_predict*0.9942 <= true_close:\n",
        "      winners += 1\n",
        "    buys += 1\n",
        "    #print('Bought at:', str(actual_predict*0.9942, ' Sold at:', str(true_close), ' Current Acc Value:', str(acc_value))\n",
        "print('This current strategy produces', str((acc_value[0]/1000 - 1)*100)+ \"% Profit over the test timeframe\")\n",
        "print('Winrate:', winners/buys)\n",
        "print('Total Transactions:', buys)"
      ],
      "metadata": {
        "id": "Qcu-54aMrBOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}